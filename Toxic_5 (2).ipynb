{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic_5.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX73RqyxbD9p",
        "colab_type": "code",
        "outputId": "02900b2c-db3d-438e-845b-407286e1836a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JJkWN3cbgBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense,Flatten, LSTM,GRU, Conv1D,MaxPooling1D,Dropout,Activation,SpatialDropout1D,Bidirectional,BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from sklearn import metrics\n",
        "import nltk\n",
        "import string\n",
        "import statistics\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcGccDfObkYA",
        "colab_type": "code",
        "outputId": "94c46850-fe75-45cc-9e96-dd18a253160c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "path=\"/content/drive/My Drive/Toxic Project/train.csv\"\n",
        "train_data=pd.read_csv(path)\n",
        "train_data.head(10)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00025465d4725e87</td>\n",
              "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00031b1e95af7921</td>\n",
              "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00037261f536c51d</td>\n",
              "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00040093b2687caa</td>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "5  00025465d4725e87  ...             0\n",
              "6  0002bcb3da6cb337  ...             0\n",
              "7  00031b1e95af7921  ...             0\n",
              "8  00037261f536c51d  ...             0\n",
              "9  00040093b2687caa  ...             0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsWHdDMKbnVQ",
        "colab_type": "code",
        "outputId": "2a5f5255-a59e-41cd-ed69-bd7982de96cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data.columns.isna()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQngaYJbbrwl",
        "colab_type": "code",
        "outputId": "14b10a2d-14cc-4618-e7d8-9d92a40635a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "y_train=np.array(train_data.drop(['id','comment_text'],axis=1))\n",
        "y_train"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smQWG9o5bsc5",
        "colab_type": "code",
        "outputId": "47231b09-708f-4c8b-dd4d-c408dd343284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "path_test=\"/content/drive/My Drive/Toxic Project/test.csv\"\n",
        "test_data=pd.read_csv(path_test)\n",
        "test_data"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>:If you have a look back at the source, the in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>I don't anonymously edit articles at all.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153159</th>\n",
              "      <td>fffcd0960ee309b5</td>\n",
              "      <td>. \\n i totally agree, this stuff is nothing bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153160</th>\n",
              "      <td>fffd7a9a6eb32c16</td>\n",
              "      <td>== Throw from out field to home plate. == \\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153161</th>\n",
              "      <td>fffda9e8d6fafa9e</td>\n",
              "      <td>\" \\n\\n == Okinotorishima categories == \\n\\n I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153162</th>\n",
              "      <td>fffe8f1340a79fc2</td>\n",
              "      <td>\" \\n\\n == \"\"One of the founding nations of the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153163</th>\n",
              "      <td>ffffce3fb183ee80</td>\n",
              "      <td>\" \\n :::Stop already. Your bullshit is not wel...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153164 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id                                       comment_text\n",
              "0       00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
              "1       0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
              "2       00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
              "3       00017563c3f7919a  :If you have a look back at the source, the in...\n",
              "4       00017695ad8997eb          I don't anonymously edit articles at all.\n",
              "...                  ...                                                ...\n",
              "153159  fffcd0960ee309b5  . \\n i totally agree, this stuff is nothing bu...\n",
              "153160  fffd7a9a6eb32c16  == Throw from out field to home plate. == \\n\\n...\n",
              "153161  fffda9e8d6fafa9e  \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
              "153162  fffe8f1340a79fc2  \" \\n\\n == \"\"One of the founding nations of the...\n",
              "153163  ffffce3fb183ee80  \" \\n :::Stop already. Your bullshit is not wel...\n",
              "\n",
              "[153164 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeLNj3Fzbx5x",
        "colab_type": "code",
        "outputId": "fe610f05-14fa-43bb-f40e-ef68855cbcde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "test_label_path=\"/content/drive/My Drive/Toxic Project/test_labels.csv\"\n",
        "test_label=pd.read_csv(test_label_path)\n",
        "y_test=test_label.iloc[:,1:7]\n",
        "y_test[y_test==-1]=1\n",
        "y_test"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153159</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153160</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153161</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153162</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153163</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153164 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
              "0           1             1        1       1       1              1\n",
              "1           1             1        1       1       1              1\n",
              "2           1             1        1       1       1              1\n",
              "3           1             1        1       1       1              1\n",
              "4           1             1        1       1       1              1\n",
              "...       ...           ...      ...     ...     ...            ...\n",
              "153159      1             1        1       1       1              1\n",
              "153160      1             1        1       1       1              1\n",
              "153161      1             1        1       1       1              1\n",
              "153162      1             1        1       1       1              1\n",
              "153163      1             1        1       1       1              1\n",
              "\n",
              "[153164 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-17acnzBbzbR",
        "colab_type": "code",
        "outputId": "126544e0-0c26-4ba0-d1f9-e8ec9cd71701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "test_label_path=\"/content/drive/My Drive/Toxic Project/test_labels.csv\"\n",
        "test_label=pd.read_csv(test_label_path)\n",
        "y_test=test_label.iloc[:,1:7]\n",
        "y_test[y_test==-1]=1\n",
        "y_test"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153159</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153160</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153161</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153162</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153163</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153164 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
              "0           1             1        1       1       1              1\n",
              "1           1             1        1       1       1              1\n",
              "2           1             1        1       1       1              1\n",
              "3           1             1        1       1       1              1\n",
              "4           1             1        1       1       1              1\n",
              "...       ...           ...      ...     ...     ...            ...\n",
              "153159      1             1        1       1       1              1\n",
              "153160      1             1        1       1       1              1\n",
              "153161      1             1        1       1       1              1\n",
              "153162      1             1        1       1       1              1\n",
              "153163      1             1        1       1       1              1\n",
              "\n",
              "[153164 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O4p48_ycJ4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "def clean_text_sequence(x):\n",
        "    #clean punctuation \n",
        "    x=x.translate(string.punctuation)\n",
        "    \n",
        "    #convert text to lower case\n",
        "    x=x.lower()\n",
        "\n",
        "    #split the words\n",
        "    x=x.split()\n",
        "    \n",
        "    #remove stop words\n",
        "    remove_stopwords_list=set(stopwords.words(\"english\"))\n",
        "    \n",
        "    #select the threshold\n",
        "    new_word_list=[word for word in x if not word in  remove_stopwords_list and len(word)>3 ]\n",
        "\n",
        "            \n",
        "    text=\" \".join(new_word_list)\n",
        "    \n",
        "    #clean the text\n",
        "    #extracted by https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    #remove stemming i.e. overfitting>>>>overfit\n",
        "    \n",
        "    text_new=text.split()\n",
        "    stemmer_type=SnowballStemmer('english')\n",
        "    stemmed_words=[stemmer_type.stem(word) for word in text_new ]\n",
        "    \n",
        "#     for word in text_new:\n",
        "#         stemmed_words=[]\n",
        "#         stemmed_words.append(stemmer_type.stem(word))\n",
        "        \n",
        "    text_stemmed=\" \".join(stemmed_words)\n",
        "    \n",
        "    return text_stemmed\n",
        "    \n",
        "            \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6ysYpJScMqr",
        "colab_type": "code",
        "outputId": "ebfbde36-a3a5-4c47-cfaa-b1f494b580d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pdg5qGfcSay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=train_data['comment_text'].map(lambda x:clean_text_sequence(x) )\n",
        "x_test=test_data['comment_text'].map(lambda x:clean_text_sequence(x) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33pkGnHfcUSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unpolite_word_path=\"/content/drive/My Drive/Toxic Project/unpolite_word_list1.txt\"\n",
        "unpolite_words=pd.read_csv(unpolite_word_path,encoding='utf-8')\n",
        "unpolite_words_list=unpolite_words.iloc[:,0].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esEx7L6Ml9Yh",
        "colab_type": "code",
        "outputId": "d1faa773-72ab-4ba4-c34f-f8c25cc7c215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt install -qq enchant\n",
        "!pip install pyenchant"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 1,310 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 145674 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Collecting pyenchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/54/04d88a59efa33fefb88133ceb638cdf754319030c28aadc5a379d82140ed/pyenchant-2.0.0.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyenchant\n",
            "  Building wheel for pyenchant (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyenchant: filename=pyenchant-2.0.0-cp36-none-any.whl size=71113 sha256=863411c25e8e7acdff0c0edc8ca92f5d54b152c6400e54f31053f82754de7bf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/8e/01/f427e9c6c0ae5e22095f3d2aac8997abe7b317307a9de497f4\n",
            "Successfully built pyenchant\n",
            "Installing collected packages: pyenchant\n",
            "Successfully installed pyenchant-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH0ApyfznGNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check the x_train sequence is in English or not. Non English words are excluded\n",
        "import enchant\n",
        "new_x_train_list=[]\n",
        "dict = enchant.Dict(\"en_US\")\n",
        "\n",
        "for x in x_train:\n",
        "  temp_seq=\"\"\n",
        "  temp_word_list = x.split(\" \")\n",
        "  for y in range(len(temp_word_list)):\n",
        "    if not temp_word_list[y]:\n",
        "      continue\n",
        "    if temp_word_list[y] in unpolite_words_list:\n",
        "      temp_seq=temp_seq+temp_word_list[y] + \" \"\n",
        "    elif dict.check(temp_word_list[y]):\n",
        "      temp_seq=temp_seq+temp_word_list[y] + \" \"\n",
        "  new_x_train_list.append(temp_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD9p4C2gp5x2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check the x_test sequence is in English or not. Non English words are excluded\n",
        "import enchant\n",
        "new_x_test_list=[]\n",
        "dict = enchant.Dict(\"en_US\")\n",
        "\n",
        "for x in x_test:\n",
        "  temp_seq=\"\"\n",
        "  temp_word_list = x.split(\" \")\n",
        "  for y in range(len(temp_word_list)):\n",
        "    if not temp_word_list[y]:\n",
        "      continue\n",
        "    if temp_word_list[y] in unpolite_words_list:\n",
        "      temp_seq=temp_seq+temp_word_list[y] + \" \"\n",
        "    elif dict.check(temp_word_list[y]):\n",
        "      temp_seq=temp_seq+temp_word_list[y] + \" \"\n",
        "  new_x_test_list.append(temp_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfT47kt0nLe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_words=5000 #maximum number of words for embedding\n",
        "tokenizer=Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(new_x_train_list)\n",
        "tokenizer.fit_on_texts(new_x_test_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG4xKSGJnS0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_seq = tokenizer.texts_to_sequences(new_x_train_list)\n",
        "x_test_seq = tokenizer.texts_to_sequences(new_x_test_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypajg8aJnV7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_size = len(tokenizer.word_index) + 1   \n",
        "max_length = 200 #maxmimum number of words for a sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BufcDkgAnYCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_pad = pad_sequences(x_train_seq, padding='post', maxlen=max_length)\n",
        "x_test_pad = pad_sequences(x_test_seq, padding='post', maxlen=max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02gxoVd4naSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "size=len(unpolite_words_list)\n",
        "\n",
        "count=0\n",
        "word_list_temp=unpolite_words_list\n",
        "for w, index in word_index.items():\n",
        "    if w in unpolite_words_list:\n",
        "        word_list_temp.remove(w)\n",
        "        count+=1 \n",
        "    if index > (unique_words-size+count):\n",
        "        for u_w in word_list_temp:\n",
        "            tokenizer.word_index[u_w] = index\n",
        "            index+=1\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkVnyrkZneLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_glove=\"/content/drive/My Drive/Toxic Project/glove.6B.300d.txt\"\n",
        "embeddings_word_dict = {}\n",
        "with open(path_glove, 'r',encoding=\"utf8\") as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_word_dict [word] = vector\n",
        "\n",
        "#match the embedding vector into the relavant word index\n",
        "embedding_word_vector = np.zeros((word_size, 300))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedd_vector = embeddings_word_dict .get(word)\n",
        "    if embedd_vector is not None:\n",
        "        embedding_word_vector [index] = embedd_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VMML7ZDngz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train1,x_val,y_train1,y_val=train_test_split(x_train_pad,y_train,random_state=123,test_size=0.33)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3N5-au7njQW",
        "colab_type": "code",
        "outputId": "10a35e9e-007b-4f62-b2b0-86a2c941ed85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "model14 = Sequential()\n",
        "model14.add(Embedding(word_size, 300,weights=[embedding_word_vector], input_length=max_length))\n",
        "model14.add(SpatialDropout1D(0.2))\n",
        "model14.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2,activation='tanh',return_sequences=True)))\n",
        "# model14.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2,activation='tanh',return_sequences=True)))\n",
        "model14.add(Bidirectional(GRU((512), dropout=0.2, recurrent_dropout=0.2,activation='tanh',return_sequences=True)))\n",
        "model14.add(Bidirectional(GRU((512), dropout=0.2, recurrent_dropout=0.2,activation='tanh')))\n",
        "model14.add(Dense(64, activation='relu'))\n",
        "model14.add(Dense(64, activation='relu'))\n",
        "model14.add(Dense(6, activation='sigmoid'))\n",
        "model14.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model14.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 200, 300)          7631700   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 200, 512)          1140736   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 200, 1024)         3148800   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 1024)              4721664   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                65600     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 16,713,050\n",
            "Trainable params: 16,713,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13LpwFaqjnod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_checkpoint = ModelCheckpoint('/content/drive/My Drive/Toxic Project/model14_toxic_4.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "callback = EarlyStopping(monitor='val_loss',mode='min', verbose=1, patience=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RidhCOkrnxs_",
        "colab_type": "code",
        "outputId": "9d7c682b-75e7-4768-b40a-4b2abc50c538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "model14.fit(x_train1, y_train1, batch_size=512, epochs=10, verbose=2, validation_data=(x_val, y_val),\n",
        "         callbacks=[callback,model_checkpoint],class_weight = 'auto')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 106912 samples, validate on 52659 samples\n",
            "Epoch 1/10\n",
            " - 828s - loss: 0.0922 - acc: 0.9686 - val_loss: 0.0566 - val_acc: 0.9804\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.05663, saving model to /content/drive/My Drive/Toxic Project/model14_toxic_4.h5\n",
            "Epoch 2/10\n",
            " - 794s - loss: 0.0580 - acc: 0.9799 - val_loss: 0.0548 - val_acc: 0.9808\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.05663 to 0.05479, saving model to /content/drive/My Drive/Toxic Project/model14_toxic_4.h5\n",
            "Epoch 3/10\n",
            " - 795s - loss: 0.0550 - acc: 0.9807 - val_loss: 0.0558 - val_acc: 0.9810\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.05479\n",
            "Epoch 4/10\n",
            " - 795s - loss: 0.0530 - acc: 0.9813 - val_loss: 0.0535 - val_acc: 0.9813\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.05479 to 0.05349, saving model to /content/drive/My Drive/Toxic Project/model14_toxic_4.h5\n",
            "Epoch 5/10\n",
            " - 794s - loss: 0.0512 - acc: 0.9817 - val_loss: 0.0532 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.05349 to 0.05318, saving model to /content/drive/My Drive/Toxic Project/model14_toxic_4.h5\n",
            "Epoch 6/10\n",
            " - 794s - loss: 0.0498 - acc: 0.9818 - val_loss: 0.0527 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.05318 to 0.05267, saving model to /content/drive/My Drive/Toxic Project/model14_toxic_4.h5\n",
            "Epoch 7/10\n",
            " - 795s - loss: 0.0479 - acc: 0.9825 - val_loss: 0.0537 - val_acc: 0.9809\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.05267\n",
            "Epoch 8/10\n",
            " - 794s - loss: 0.0462 - acc: 0.9831 - val_loss: 0.0535 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.05267\n",
            "Epoch 9/10\n",
            " - 795s - loss: 0.0446 - acc: 0.9835 - val_loss: 0.0547 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.05267\n",
            "Epoch 10/10\n",
            " - 794s - loss: 0.0425 - acc: 0.9841 - val_loss: 0.0572 - val_acc: 0.9809\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.05267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f621acc83c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRkdMshNn7_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model14.save(\"/content/drive/My Drive/Toxic Project/model14_toxic_4.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxWFGKtQoi--",
        "colab_type": "code",
        "outputId": "73971b91-176b-40ca-ca0e-6c53eb8ed5d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "new_model14=load_model(\"/content/drive/My Drive/Toxic Project/model14_toxic_4.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwN8ehY6ooYp",
        "colab_type": "code",
        "outputId": "dc53b1e1-9c76-48ac-b488-9aa97d71faff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "predict_y_val14=new_model14.predict(x_val)\n",
        "predict_y_test14=new_model14.predict(x_test_pad)\n",
        "\n",
        "col_nammes=['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
        "y_validation_val=[]\n",
        "y_test_val=[]\n",
        "for i,col in enumerate(col_nammes):\n",
        "  roc_value_y_validation=metrics.roc_auc_score(y_val[:,i],predict_y_val14[:,i])\n",
        "  roc_value_y_test=metrics.roc_auc_score(y_test[col],predict_y_test14[:,i])\n",
        "\n",
        "  print(col,\"Validation_accuracy:\",roc_value_y_validation,\"\\t Test Accuracy:\",roc_value_y_test,\"\\n\")\n",
        "\n",
        "  y_validation_val.append(roc_value_y_validation)\n",
        "  y_test_val.append(roc_value_y_test)\n",
        "  \n",
        "Accuracy_y_validation=statistics.mean(y_validation_val)\n",
        "Accuracy_y_test=statistics.mean(y_test_val)\n",
        "print(\"Accuracy_y_validation\",Accuracy_y_validation)\n",
        "print(\"Accuracy_y_test\",Accuracy_y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "toxic Validation_accuracy: 0.9607315216164053 \t Test Accuracy: 0.6338936749861988 \n",
            "\n",
            "severe_toxic Validation_accuracy: 0.9869768625577664 \t Test Accuracy: 0.5743000335980378 \n",
            "\n",
            "obscene Validation_accuracy: 0.9812787882246776 \t Test Accuracy: 0.6133553174767506 \n",
            "\n",
            "threat Validation_accuracy: 0.9730431765315487 \t Test Accuracy: 0.5673469742706546 \n",
            "\n",
            "insult Validation_accuracy: 0.9737751803815262 \t Test Accuracy: 0.6091390880514789 \n",
            "\n",
            "identity_hate Validation_accuracy: 0.9597736413932145 \t Test Accuracy: 0.5757019450756822 \n",
            "\n",
            "Accuracy_y_validation 0.9725965284508564\n",
            "Accuracy_y_test 0.5956228389098005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyrNoV4pT9G_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "template = pd.read_csv('/content/drive/My Drive/Toxic Project/sample_submission.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVInaKRJMHT_",
        "colab_type": "code",
        "outputId": "a543f5ec-8dbe-4d1f-f248-2c1145a88a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "template"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153159</th>\n",
              "      <td>fffcd0960ee309b5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153160</th>\n",
              "      <td>fffd7a9a6eb32c16</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153161</th>\n",
              "      <td>fffda9e8d6fafa9e</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153162</th>\n",
              "      <td>fffe8f1340a79fc2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153163</th>\n",
              "      <td>ffffce3fb183ee80</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153164 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id  toxic  severe_toxic  ...  threat  insult  identity_hate\n",
              "0       00001cee341fdb12    0.5           0.5  ...     0.5     0.5            0.5\n",
              "1       0000247867823ef7    0.5           0.5  ...     0.5     0.5            0.5\n",
              "2       00013b17ad220c46    0.5           0.5  ...     0.5     0.5            0.5\n",
              "3       00017563c3f7919a    0.5           0.5  ...     0.5     0.5            0.5\n",
              "4       00017695ad8997eb    0.5           0.5  ...     0.5     0.5            0.5\n",
              "...                  ...    ...           ...  ...     ...     ...            ...\n",
              "153159  fffcd0960ee309b5    0.5           0.5  ...     0.5     0.5            0.5\n",
              "153160  fffd7a9a6eb32c16    0.5           0.5  ...     0.5     0.5            0.5\n",
              "153161  fffda9e8d6fafa9e    0.5           0.5  ...     0.5     0.5            0.5\n",
              "153162  fffe8f1340a79fc2    0.5           0.5  ...     0.5     0.5            0.5\n",
              "153163  ffffce3fb183ee80    0.5           0.5  ...     0.5     0.5            0.5\n",
              "\n",
              "[153164 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtsMFv1wMAX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_y_test14=new_model14.predict(x_test_pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvP1ulmYjL-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "dd3e71b3-a099-47a0-f0fe-239ee38f3f56"
      },
      "source": [
        "predict_y_test14"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.92672741e-01, 4.18185771e-01, 9.34466898e-01, 3.13995153e-01,\n",
              "        8.39612126e-01, 4.79414463e-01],\n",
              "       [7.63203502e-02, 2.17297673e-03, 4.82550263e-02, 1.67274475e-03,\n",
              "        4.89917099e-02, 8.74680281e-03],\n",
              "       [1.03207976e-01, 4.30911779e-03, 4.83199656e-02, 1.92752481e-03,\n",
              "        5.82358241e-02, 9.49963927e-03],\n",
              "       ...,\n",
              "       [8.52862000e-03, 2.15172768e-05, 1.17138028e-03, 4.91440296e-05,\n",
              "        1.40422583e-03, 1.77353621e-04],\n",
              "       [5.57008386e-03, 1.65402889e-05, 7.96914101e-04, 3.09348106e-05,\n",
              "        1.01467967e-03, 1.39176846e-04],\n",
              "       [9.38155830e-01, 1.26451552e-02, 6.05323315e-01, 5.28299809e-03,\n",
              "        5.25995791e-01, 1.82274282e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BamPJqccpqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.DataFrame(predict_y_test14,columns=['toxic',\t'severe_toxic',\t'obscene',\t'threat',\t'insult',\t'identity_hate'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUOThHV0hZm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.insert(loc=0, column='id', value=test_data['id'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LuleU0OiBKP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "50edd8e5-cc99-4686-d194-af33374c7ea1"
      },
      "source": [
        "df"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>0.992673</td>\n",
              "      <td>0.418186</td>\n",
              "      <td>0.934467</td>\n",
              "      <td>0.313995</td>\n",
              "      <td>0.839612</td>\n",
              "      <td>0.479414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>0.076320</td>\n",
              "      <td>0.002173</td>\n",
              "      <td>0.048255</td>\n",
              "      <td>0.001673</td>\n",
              "      <td>0.048992</td>\n",
              "      <td>0.008747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>0.103208</td>\n",
              "      <td>0.004309</td>\n",
              "      <td>0.048320</td>\n",
              "      <td>0.001928</td>\n",
              "      <td>0.058236</td>\n",
              "      <td>0.009500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>0.033721</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.014239</td>\n",
              "      <td>0.000717</td>\n",
              "      <td>0.010914</td>\n",
              "      <td>0.001794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153159</th>\n",
              "      <td>fffcd0960ee309b5</td>\n",
              "      <td>0.698828</td>\n",
              "      <td>0.005148</td>\n",
              "      <td>0.321064</td>\n",
              "      <td>0.001329</td>\n",
              "      <td>0.132139</td>\n",
              "      <td>0.006734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153160</th>\n",
              "      <td>fffd7a9a6eb32c16</td>\n",
              "      <td>0.004604</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000868</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.000109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153161</th>\n",
              "      <td>fffda9e8d6fafa9e</td>\n",
              "      <td>0.008529</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.001171</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.001404</td>\n",
              "      <td>0.000177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153162</th>\n",
              "      <td>fffe8f1340a79fc2</td>\n",
              "      <td>0.005570</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.001015</td>\n",
              "      <td>0.000139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153163</th>\n",
              "      <td>ffffce3fb183ee80</td>\n",
              "      <td>0.938156</td>\n",
              "      <td>0.012645</td>\n",
              "      <td>0.605323</td>\n",
              "      <td>0.005283</td>\n",
              "      <td>0.525996</td>\n",
              "      <td>0.018227</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153164 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id     toxic  ...    insult  identity_hate\n",
              "0       00001cee341fdb12  0.992673  ...  0.839612       0.479414\n",
              "1       0000247867823ef7  0.076320  ...  0.048992       0.008747\n",
              "2       00013b17ad220c46  0.103208  ...  0.058236       0.009500\n",
              "3       00017563c3f7919a  0.000990  ...  0.000218       0.000032\n",
              "4       00017695ad8997eb  0.033721  ...  0.010914       0.001794\n",
              "...                  ...       ...  ...       ...            ...\n",
              "153159  fffcd0960ee309b5  0.698828  ...  0.132139       0.006734\n",
              "153160  fffd7a9a6eb32c16  0.004604  ...  0.000875       0.000109\n",
              "153161  fffda9e8d6fafa9e  0.008529  ...  0.001404       0.000177\n",
              "153162  fffe8f1340a79fc2  0.005570  ...  0.001015       0.000139\n",
              "153163  ffffce3fb183ee80  0.938156  ...  0.525996       0.018227\n",
              "\n",
              "[153164 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nne3-MYUieQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"/content/drive/My Drive/Toxic Project/results1.csv\", index=True, sep=\";\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6mJpiU_lEEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}